{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa training from skretch made easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaqube.nlp.roberta.seaberta import SeaBERTa\n",
    "from seaqube.nlp.types import SeaQuBeWordEmbeddingsModel, SeaQuBeNLPModel2WV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaqube.tools.io import load_json, save_json\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some seaqube tools:\n",
    "from seaqube.nlp.tools import word_count_list\n",
    "from seaqube.nlp.types import RawModelTinCan\n",
    "from seaqube.nlp.seaqube_model import SeaQuBeNLPLoader, SeaQuBeCompressLoader\n",
    "from seaqube.nlp.tools import tokenize_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoda_cites = [[\"fear\", \"is\", \"the\", \"path\", \"to\", \"the\", \"dark\", \"side\", \".\", \"fear\", \"leads\", \"to\", \"anger\", \".\", \"anger\", \"leads\", \"to\", \"hate\", \".\", \"hate\", \"leads\", \"to\", \"suffering\", \".\"], [\"once\", \"you\", \"start\", \"down\", \"the\", \"dark\", \"path\", \",\", \"forever\", \"will\", \"it\", \"dominate\", \"your\", \"destiny\", \".\", \"consume\", \"you\", \",\", \"it\", \"will\", \".\"], [\"always\", \"pass\", \"on\", \"what\", \"you\", \"have\", \"learned\", \".\"], [\"patience\", \"you\", \"must\", \"have\", \"my\", \"young\", \"padawan\", \".\"], [\"in\", \"a\", \"dark\", \"place\", \"we\", \"find\", \"ourselves\", \",\", \"and\", \"a\", \"little\", \"more\", \"knowledge\", \"lights\", \"our\", \"way\", \".\"], [\"death\", \"is\", \"a\", \"natural\", \"part\", \"of\", \"life\", \".\", \"rejoice\", \"for\", \"those\", \"around\", \"you\", \"who\", \"transform\", \"into\", \"the\", \"force\", \".\", \"mourn\", \"them\", \"do\", \"not\", \".\", \"miss\", \"them\", \"do\", \"not\", \".\", \"attachment\", \"leads\", \"to\", \"jealously\", \".\", \"the\", \"shadow\", \"of\", \"greed\", \",\", \"that\", \"is\", \".\"], [\"powerful\", \"you\", \"have\", \"become\", \",\", \"the\", \"dark\", \"side\", \"i\", \"sense\", \"in\", \"you\", \".\"], [\"train\", \"yourself\", \"to\", \"let\", \"go\", \"of\", \"everything\", \"you\", \"fear\", \"to\", \"lose\", \".\"], [\"feel\", \"the\", \"force\", \"!\"], [\"truly\", \"wonderful\", \"the\", \"mind\", \"of\", \"a\", \"child\", \"is\", \".\"], [\"do\", \"or\", \"do\", \"not\", \".\", \"there\", \"is\", \"no\", \"try\", \".\"], [\"great\", \"warrior\", \".\", \"wars\", \"not\", \"make\", \"one\", \"great\", \".\"], [\"size\", \"matters\", \"not\", \".\", \"look\", \"at\", \"me\", \".\", \"judge\", \"me\", \"by\", \"my\", \"size\", \",\", \"do\", \"you\", \"?\", \"hmm\", \"?\", \"hmm\", \".\", \"and\", \"well\", \"you\", \"should\", \"not\", \".\", \"for\", \"my\", \"ally\", \"is\", \"the\", \"force\", \",\", \"and\", \"a\", \"powerful\", \"ally\", \"it\", \"is\", \".\", \"life\", \"creates\", \"it\", \",\", \"makes\", \"it\", \"grow\", \".\", \"its\", \"energy\", \"surrounds\", \"us\", \"and\", \"binds\", \"us\", \".\", \"luminous\", \"beings\", \"are\", \"we\", \",\", \"not\", \"this\", \"crude\", \"matter\", \".\", \"you\", \"must\", \"feel\", \"the\", \"force\", \"around\", \"you\", \";\", \"here\", \",\", \"between\", \"you\", \",\", \"me\", \",\", \"the\", \"tree\", \",\", \"the\", \"rock\", \",\", \"everywhere\", \",\", \"yes\", \".\", \"even\", \"between\", \"the\", \"land\", \"and\", \"the\", \"ship\", \".\"], [\"the\", \"dark\", \"side\", \"clouds\", \"everything\", \".\", \"impossible\", \"to\", \"see\", \"the\", \"light\", \",\", \"the\", \"future\", \"is\", \".\"], [\"you\", \"will\", \"find\", \"only\", \"what\", \"you\", \"bring\", \"in\", \".\"], [\"to\", \"be\", \"jedi\", \"is\", \"to\", \"face\", \"the\", \"truth\", \",\", \"and\", \"choose\", \".\", \"give\", \"off\", \"light\", \",\", \"or\", \"darkness\", \",\", \"padawan\", \".\", \"be\", \"a\", \"candle\", \",\", \"or\", \"the\", \"night\", \".\"], [\"control\", \",\", \"control\", \",\", \"you\", \"must\", \"learn\", \"control\", \"!\"], [\"on\", \"many\", \"long\", \"journeys\", \"have\", \"i\", \"gone\", \".\", \"and\", \"waited\", \",\", \"too\", \",\", \"for\", \"others\", \"to\", \"return\", \"from\", \"journeys\", \"of\", \"their\", \"own\", \".\", \"some\", \"return\", \";\", \"some\", \"are\", \"broken\", \";\", \"some\", \"come\", \"back\", \"so\", \"different\", \"only\", \"their\", \"names\", \"remain\", \".\"], [\"in\", \"the\", \"end\", \",\", \"cowards\", \"are\", \"those\", \"who\", \"follow\", \"the\", \"dark\", \"side\", \".\"], [\"difficult\", \"to\", \"see\", \".\", \"always\", \"in\", \"motion\", \"is\", \"the\", \"future\", \".\"], [\"ready\", \"are\", \"you\", \"?\", \"what\", \"know\", \"you\", \"of\", \"ready\", \"?\", \"for\", \"eight\", \"hundred\", \"years\", \"have\", \"i\", \"trained\", \"jedi\", \".\", \"my\", \"own\", \"counsel\", \"will\", \"i\", \"keep\", \"on\", \"who\", \"is\", \"to\", \"be\", \"trained\", \".\", \"a\", \"jedi\", \"must\", \"have\", \"the\", \"deepest\", \"commitment\", \",\", \"the\", \"most\", \"serious\", \"mind\", \".\", \"this\", \"one\", \"a\", \"long\", \"time\", \"have\", \"i\", \"watched\", \".\", \"all\", \"his\", \"life\", \"has\", \"he\", \"looked\", \"away\\u2026\", \"to\", \"the\", \"future\", \",\", \"to\", \"the\", \"horizon\", \".\", \"never\", \"his\", \"mind\", \"on\", \"where\", \"he\", \"was\", \".\", \"hmm\", \"?\", \"what\", \"he\", \"was\", \"doing\", \".\", \"hmph\", \".\", \"adventure\", \".\", \"heh\", \".\", \"excitement\", \".\", \"heh\", \".\", \"a\", \"jedi\", \"craves\", \"not\", \"these\", \"things\", \".\", \"you\", \"are\", \"reckless\", \".\"], [\"secret\", \",\", \"shall\", \"i\", \"tell\", \"you\", \"?\", \"grand\", \"master\", \"of\", \"jedi\", \"order\", \"am\", \"i\", \".\", \"won\", \"this\", \"job\", \"in\", \"a\", \"raffle\", \"i\", \"did\", \",\", \"think\", \"you\", \"?\", \"\\u2018\", \"how\", \"did\", \"you\", \"know\", \",\", \"how\", \"did\", \"you\", \"know\", \",\", \"master\", \"yoda\", \"?\", \"\\u2019\", \"master\", \"yoda\", \"knows\", \"these\", \"things\", \".\", \"his\", \"job\", \"it\", \"is\", \".\"], [\"to\", \"answer\", \"power\", \"with\", \"power\", \",\", \"the\", \"jedi\", \"way\", \"this\", \"is\", \"not\", \".\", \"in\", \"this\", \"war\", \",\", \"a\", \"danger\", \"there\", \"is\", \",\", \"of\", \"losing\", \"who\", \"we\", \"are\", \".\"], [\"many\", \"of\", \"the\", \"truths\", \"that\", \"we\", \"cling\", \"to\", \"depend\", \"on\", \"our\", \"point\", \"of\", \"view\", \".\"], [\"named\", \"must\", \"your\", \"fear\", \"be\", \"before\", \"banish\", \"it\", \"you\", \"can\", \".\"], [\"you\", \"think\", \"yoda\", \"stops\", \"teaching\", \",\", \"just\", \"because\", \"his\", \"student\", \"does\", \"not\", \"want\", \"to\", \"hear\", \"?\", \"a\", \"teacher\", \"yoda\", \"is\", \".\", \"yoda\", \"teaches\", \"like\", \"drunkards\", \"drink\", \",\", \"like\", \"killers\", \"kill\", \".\"], [\"do\", \"not\", \"assume\", \"anything\", \"obi-wan\", \".\", \"clear\", \"your\", \"mind\", \"must\", \"be\", \"if\", \"you\", \"are\", \"to\", \"discover\", \"the\", \"real\", \"villains\", \"behind\", \"this\", \"plot\", \".\"], [\"you\", \"will\", \"know\", \"(\", \"the\", \"good\", \"from\", \"the\", \"bad\", \")\", \"when\", \"you\", \"are\", \"calm\", \",\", \"at\", \"peace\", \".\", \"passive\", \".\", \"a\", \"jedi\", \"uses\", \"the\", \"force\", \"for\", \"knowledge\", \"and\", \"defense\", \",\", \"never\", \"for\", \"attack\", \".\"], [\"soon\", \"will\", \"i\", \"rest\", \",\", \"yes\", \",\", \"forever\", \"sleep\", \".\", \"earned\", \"it\", \"i\", \"have\", \".\", \"twilight\", \"is\", \"upon\", \"me\", \",\", \"soon\", \"night\", \"must\", \"fall\", \".\"], [\"when\", \"you\", \"look\", \"at\", \"the\", \"dark\", \"side\", \",\", \"careful\", \"you\", \"must\", \"be\", \".\", \"for\", \"the\", \"dark\", \"side\", \"looks\", \"back\", \".\"], [\"you\", \"will\", \"know\", \"(\", \"the\", \"good\", \"from\", \"the\", \"bad\", \")\", \"when\", \"you\", \"are\", \"calm\", \",\", \"at\", \"peace\", \".\", \"passive\", \".\", \"a\", \"jedi\", \"uses\", \"the\", \"force\", \"for\", \"knowledge\", \"and\", \"defense\", \",\", \"never\", \"for\", \"attack\", \".\"], [\"smaller\", \"in\", \"number\", \"are\", \"we\", \",\", \"but\", \"larger\", \"in\", \"mind\", \".\"], [\"your\", \"path\", \"you\", \"must\", \"decide\", \".\"], [\"always\", \"two\", \"there\", \"are\", \",\", \"no\", \"more\", \",\", \"no\", \"less\", \".\", \"a\", \"master\", \"and\", \"an\", \"apprentice\", \".\"], [\"no\", \"longer\", \"certain\", \",\", \"that\", \"one\", \"ever\", \"does\", \"win\", \"a\", \"war\", \",\", \"i\", \"am\", \".\", \"for\", \"in\", \"fighting\", \"the\", \"battles\", \",\", \"the\", \"bloodshed\", \",\", \"already\", \"lost\", \"we\", \"have\", \".\", \"yet\", \",\", \"open\", \"to\", \"us\", \"a\", \"path\", \"remains\", \".\", \"that\", \"unknown\", \"to\", \"the\", \"sith\", \"is\", \".\", \"through\", \"this\", \"path\", \",\", \"victory\", \"we\", \"may\", \"yet\", \"find\", \".\", \"not\", \"victory\", \"in\", \"the\", \"clone\", \"wars\", \",\", \"but\", \"victory\", \"for\", \"all\", \"time\", \".\"], [\"if\", \"no\", \"mistake\", \"you\", \"have\", \"made\", \",\", \"losing\", \"you\", \"are\", \".\", \"a\", \"different\", \"game\", \"you\", \"should\", \"play\", \".\"], [\"[\", \"luke\", \"skywalker\", \":\", \"]\", \"i\", \"can\", \"\\u2019\", \"t\", \"believe\", \"it\", \".\", \"[\", \"yoda\", \":\", \"]\", \"that\", \"is\", \"why\", \"you\", \"fail\", \".\"], [\"happens\", \"to\", \"every\", \"guy\", \"sometimes\", \"this\", \"does\"], [\"adventure\", \".\", \"excitement\", \".\", \"a\", \"jedi\", \"craves\", \"not\", \"these\", \"things\", \".\"], [\"only\", \"the\", \"dark\", \"lord\", \"of\", \"the\", \"sith\", \"knows\", \"of\", \"our\", \"weakness\", \".\", \"if\", \"informed\", \"the\", \"senate\", \"is\", \",\", \"multiply\", \"our\", \"adversaries\", \"will\", \".\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For roberta a location is needed where checkpoints and pre processing is saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = join(example_path[0], \"seaberta\", \"main\")\n",
    "train_path = join(example_path[0], \"seaberta\", \"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta needs some parameters. At least use 20 epochs for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "        \"per_gpu_eval_batch_size\": 4,\n",
    "        \"do_eval\": True,\n",
    "        \"evaluate_during_training\": False,\n",
    "        \"line_by_line\": False,\n",
    "        \"should_continue\": False,\n",
    "        \"model_name_or_path\": False,\n",
    "        \"mlm\": True,\n",
    "        \"do_train\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"overwrite_cache\": False,\n",
    "        \"block_size\": 512,\n",
    "        \"eval_all_checkpoints\": 2,\n",
    "        \"server_ip\": \"\",\n",
    "        \"mlm_probability\": 0.15,\n",
    "        \"local_rank\": -1,  # NO GPU,\n",
    "        \"no_cuda\": False,\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": 'O1',\n",
    "        \"max_steps\": 10,\n",
    "        \"warmup_steps\": 10,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"per_gpu_train_batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"max_grad_norm\": 100.0,\n",
    "        \"save_total_limit\": 10,\n",
    "        \"save_steps\": 10,\n",
    "        \"logging_steps\": 2,\n",
    "        \"seed\": 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = SeaBERTa(main_path, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:seaqube.nlp.roberta.roberta_training:Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Training new model from scratch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main.txt 1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/modeling_auto.py:797: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Training/evaluation parameters {'per_gpu_eval_batch_size': 4, 'do_eval': True, 'evaluate_during_training': False, 'line_by_line': False, 'should_continue': False, 'model_name_or_path': False, 'mlm': True, 'do_train': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'block_size': 512, 'eval_all_checkpoints': 2, 'server_ip': '', 'mlm_probability': 0.15, 'local_rank': -1, 'no_cuda': False, 'fp16': False, 'fp16_opt_level': 'O1', 'max_steps': 10, 'warmup_steps': 10, 'learning_rate': 5e-05, 'per_gpu_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'weight_decay': 0.01, 'adam_epsilon': 1e-06, 'max_grad_norm': 100.0, 'save_total_limit': 10, 'save_steps': 10, 'logging_steps': 2, 'seed': 0, 'output_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output', 'model_type': 'roberta', 'config_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'tokenizer_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'train_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_train.txt', 'eval_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_eval.txt', 'cache_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/cache', 'n_gpu': 0, 'device': device(type='cpu')}\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Loading features from cached file /Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/roberta_cached_lm_510_main_train.txt\n",
      "INFO:seaqube.nlp.roberta.roberta_training:***** Running training *****\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Num examples = 47\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Num Epochs = 2\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Instantaneous batch size per GPU = 4\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Gradient Accumulation steps = 4\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Total optimization steps = 10\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/modeling_roberta.py:888: FutureWarning: The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer PreTrainedTokenizer(name_or_path='/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', vocab_size=1017, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   8%|▊         | 1/12 [00:21<03:51, 21.08s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 2/12 [00:42<03:31, 21.19s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 3/12 [01:01<03:05, 20.63s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 4/12 [01:21<02:43, 20.40s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 5/12 [01:44<02:27, 21.07s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 6/12 [02:04<02:03, 20.67s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 7/12 [02:30<01:51, 22.38s/it]\u001b[A/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  67%|██████▋   | 8/12 [02:51<01:27, 21.99s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 9/12 [03:11<01:03, 21.29s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 10/12 [03:32<00:42, 21.32s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 11/12 [03:51<00:20, 20.71s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 12/12 [04:07<00:00, 20.61s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 1/2 [04:07<04:07, 247.38s/it]\n",
      "Iteration:   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 1/12 [00:23<04:13, 23.01s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 2/12 [00:45<03:48, 22.89s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 3/12 [01:05<03:18, 22.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 4/12 [01:26<02:52, 21.57s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 5/12 [01:45<02:26, 20.88s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 6/12 [02:05<02:04, 20.69s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 7/12 [02:26<01:43, 20.80s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 8/12 [02:46<01:22, 20.52s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 9/12 [03:05<01:00, 20.07s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 10/12 [03:24<00:39, 19.75s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 11/12 [03:43<00:19, 19.51s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 12/12 [03:58<00:00, 19.85s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 2/2 [08:05<00:00, 242.78s/it]\n",
      "INFO:seaqube.nlp.roberta.roberta_training: global_step = 6, average loss = 6.786967515945435\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Saving model checkpoint to /Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output\n",
      "/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/modeling_auto.py:837: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Evaluate the following checkpoints: ['/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG {'per_gpu_eval_batch_size': 4, 'do_eval': True, 'evaluate_during_training': False, 'line_by_line': False, 'should_continue': False, 'model_name_or_path': False, 'mlm': True, 'do_train': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'block_size': 512, 'eval_all_checkpoints': 2, 'server_ip': '', 'mlm_probability': 0.15, 'local_rank': -1, 'no_cuda': False, 'fp16': False, 'fp16_opt_level': 'O1', 'max_steps': 10, 'warmup_steps': 10, 'learning_rate': 5e-05, 'per_gpu_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'weight_decay': 0.01, 'adam_epsilon': 1e-06, 'max_grad_norm': 100.0, 'save_total_limit': 10, 'save_steps': 10, 'logging_steps': 2, 'seed': 0, 'output_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output', 'model_type': 'roberta', 'config_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'tokenizer_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'train_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_train.txt', 'eval_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_eval.txt', 'cache_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/cache', 'n_gpu': 0, 'device': device(type='cpu'), 'train_batch_size': 4, 'num_train_epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:seaqube.nlp.roberta.roberta_training:Loading features from cached file /Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/roberta_cached_lm_510_main_eval.txt\n",
      "INFO:seaqube.nlp.roberta.roberta_training:***** Running evaluation  *****\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Num examples = 5\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Batch size = 4\n",
      "Evaluating: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n",
      "INFO:seaqube.nlp.roberta.roberta_training:***** Eval results  *****\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  perplexity = tensor(680.1933)\n"
     ]
    }
   ],
   "source": [
    "roberta.train(yoda_cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeaQuBeWordEmbeddingsModelSeaBERTa(SeaQuBeWordEmbeddingsModel):\n",
    "    def __init__(self, seaberta: SeaBERTa):\n",
    "        self.seaberta = seaberta\n",
    "\n",
    "    def vocabs(self):\n",
    "        return self.seaberta.wv.vocabs\n",
    "\n",
    "    @property\n",
    "    def wv(self):\n",
    "        return self.seaberta.wv\n",
    "\n",
    "    def word_vector(self, word):\n",
    "        return self.seaberta.wv[word]\n",
    "\n",
    "    def matrix(self):\n",
    "        return self.seaberta.wv.matrix\n",
    "\n",
    "    def context_embedding(self, words, position):\n",
    "        return self.seaberta.context_embedding(words, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaberta = SeaQuBeWordEmbeddingsModelSeaBERTa(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.context_embedding([\"t\"], 0), roberta.context_embedding([\"t\"], 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.wv.vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tin_can = RawModelTinCan(seaberta, word_count_list(yoda_cites))\n",
    "nlp = SeaQuBeNLPLoader.load_model_from_tin_can(tin_can, \"seaberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Luke is a Jedi and yoda is a master Jedi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare `jedi` with `jedi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[3],doc[9] # both are the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaqube.tools.math import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(doc[3].vector, doc[9].vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
