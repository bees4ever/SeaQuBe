{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa training from skretch made easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaqube.nlp.roberta.seaberta import SeaBERTa\n",
    "from seaqube.nlp.types import SeaQuBeWordEmbeddingsModel, SeaQuBeNLPModel2WV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaqube.tools.io import load_json, save_json\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some seaqube tools:\n",
    "from seaqube.nlp.tools import word_count_list\n",
    "from seaqube.nlp.types import RawModelTinCan\n",
    "from seaqube.nlp.seaqube_model import SeaQuBeNLPLoader, SeaQuBeCompressLoader\n",
    "from seaqube.nlp.tools import tokenize_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoda_cites = [[\"fear\", \"is\", \"the\", \"path\", \"to\", \"the\", \"dark\", \"side\", \".\", \"fear\", \"leads\", \"to\", \"anger\", \".\", \"anger\", \"leads\", \"to\", \"hate\", \".\", \"hate\", \"leads\", \"to\", \"suffering\", \".\"], [\"once\", \"you\", \"start\", \"down\", \"the\", \"dark\", \"path\", \",\", \"forever\", \"will\", \"it\", \"dominate\", \"your\", \"destiny\", \".\", \"consume\", \"you\", \",\", \"it\", \"will\", \".\"], [\"always\", \"pass\", \"on\", \"what\", \"you\", \"have\", \"learned\", \".\"], [\"patience\", \"you\", \"must\", \"have\", \"my\", \"young\", \"padawan\", \".\"], [\"in\", \"a\", \"dark\", \"place\", \"we\", \"find\", \"ourselves\", \",\", \"and\", \"a\", \"little\", \"more\", \"knowledge\", \"lights\", \"our\", \"way\", \".\"], [\"death\", \"is\", \"a\", \"natural\", \"part\", \"of\", \"life\", \".\", \"rejoice\", \"for\", \"those\", \"around\", \"you\", \"who\", \"transform\", \"into\", \"the\", \"force\", \".\", \"mourn\", \"them\", \"do\", \"not\", \".\", \"miss\", \"them\", \"do\", \"not\", \".\", \"attachment\", \"leads\", \"to\", \"jealously\", \".\", \"the\", \"shadow\", \"of\", \"greed\", \",\", \"that\", \"is\", \".\"], [\"powerful\", \"you\", \"have\", \"become\", \",\", \"the\", \"dark\", \"side\", \"i\", \"sense\", \"in\", \"you\", \".\"], [\"train\", \"yourself\", \"to\", \"let\", \"go\", \"of\", \"everything\", \"you\", \"fear\", \"to\", \"lose\", \".\"], [\"feel\", \"the\", \"force\", \"!\"], [\"truly\", \"wonderful\", \"the\", \"mind\", \"of\", \"a\", \"child\", \"is\", \".\"], [\"do\", \"or\", \"do\", \"not\", \".\", \"there\", \"is\", \"no\", \"try\", \".\"], [\"great\", \"warrior\", \".\", \"wars\", \"not\", \"make\", \"one\", \"great\", \".\"], [\"size\", \"matters\", \"not\", \".\", \"look\", \"at\", \"me\", \".\", \"judge\", \"me\", \"by\", \"my\", \"size\", \",\", \"do\", \"you\", \"?\", \"hmm\", \"?\", \"hmm\", \".\", \"and\", \"well\", \"you\", \"should\", \"not\", \".\", \"for\", \"my\", \"ally\", \"is\", \"the\", \"force\", \",\", \"and\", \"a\", \"powerful\", \"ally\", \"it\", \"is\", \".\", \"life\", \"creates\", \"it\", \",\", \"makes\", \"it\", \"grow\", \".\", \"its\", \"energy\", \"surrounds\", \"us\", \"and\", \"binds\", \"us\", \".\", \"luminous\", \"beings\", \"are\", \"we\", \",\", \"not\", \"this\", \"crude\", \"matter\", \".\", \"you\", \"must\", \"feel\", \"the\", \"force\", \"around\", \"you\", \";\", \"here\", \",\", \"between\", \"you\", \",\", \"me\", \",\", \"the\", \"tree\", \",\", \"the\", \"rock\", \",\", \"everywhere\", \",\", \"yes\", \".\", \"even\", \"between\", \"the\", \"land\", \"and\", \"the\", \"ship\", \".\"], [\"the\", \"dark\", \"side\", \"clouds\", \"everything\", \".\", \"impossible\", \"to\", \"see\", \"the\", \"light\", \",\", \"the\", \"future\", \"is\", \".\"], [\"you\", \"will\", \"find\", \"only\", \"what\", \"you\", \"bring\", \"in\", \".\"], [\"to\", \"be\", \"jedi\", \"is\", \"to\", \"face\", \"the\", \"truth\", \",\", \"and\", \"choose\", \".\", \"give\", \"off\", \"light\", \",\", \"or\", \"darkness\", \",\", \"padawan\", \".\", \"be\", \"a\", \"candle\", \",\", \"or\", \"the\", \"night\", \".\"], [\"control\", \",\", \"control\", \",\", \"you\", \"must\", \"learn\", \"control\", \"!\"], [\"on\", \"many\", \"long\", \"journeys\", \"have\", \"i\", \"gone\", \".\", \"and\", \"waited\", \",\", \"too\", \",\", \"for\", \"others\", \"to\", \"return\", \"from\", \"journeys\", \"of\", \"their\", \"own\", \".\", \"some\", \"return\", \";\", \"some\", \"are\", \"broken\", \";\", \"some\", \"come\", \"back\", \"so\", \"different\", \"only\", \"their\", \"names\", \"remain\", \".\"], [\"in\", \"the\", \"end\", \",\", \"cowards\", \"are\", \"those\", \"who\", \"follow\", \"the\", \"dark\", \"side\", \".\"], [\"difficult\", \"to\", \"see\", \".\", \"always\", \"in\", \"motion\", \"is\", \"the\", \"future\", \".\"], [\"ready\", \"are\", \"you\", \"?\", \"what\", \"know\", \"you\", \"of\", \"ready\", \"?\", \"for\", \"eight\", \"hundred\", \"years\", \"have\", \"i\", \"trained\", \"jedi\", \".\", \"my\", \"own\", \"counsel\", \"will\", \"i\", \"keep\", \"on\", \"who\", \"is\", \"to\", \"be\", \"trained\", \".\", \"a\", \"jedi\", \"must\", \"have\", \"the\", \"deepest\", \"commitment\", \",\", \"the\", \"most\", \"serious\", \"mind\", \".\", \"this\", \"one\", \"a\", \"long\", \"time\", \"have\", \"i\", \"watched\", \".\", \"all\", \"his\", \"life\", \"has\", \"he\", \"looked\", \"away\\u2026\", \"to\", \"the\", \"future\", \",\", \"to\", \"the\", \"horizon\", \".\", \"never\", \"his\", \"mind\", \"on\", \"where\", \"he\", \"was\", \".\", \"hmm\", \"?\", \"what\", \"he\", \"was\", \"doing\", \".\", \"hmph\", \".\", \"adventure\", \".\", \"heh\", \".\", \"excitement\", \".\", \"heh\", \".\", \"a\", \"jedi\", \"craves\", \"not\", \"these\", \"things\", \".\", \"you\", \"are\", \"reckless\", \".\"], [\"secret\", \",\", \"shall\", \"i\", \"tell\", \"you\", \"?\", \"grand\", \"master\", \"of\", \"jedi\", \"order\", \"am\", \"i\", \".\", \"won\", \"this\", \"job\", \"in\", \"a\", \"raffle\", \"i\", \"did\", \",\", \"think\", \"you\", \"?\", \"\\u2018\", \"how\", \"did\", \"you\", \"know\", \",\", \"how\", \"did\", \"you\", \"know\", \",\", \"master\", \"yoda\", \"?\", \"\\u2019\", \"master\", \"yoda\", \"knows\", \"these\", \"things\", \".\", \"his\", \"job\", \"it\", \"is\", \".\"], [\"to\", \"answer\", \"power\", \"with\", \"power\", \",\", \"the\", \"jedi\", \"way\", \"this\", \"is\", \"not\", \".\", \"in\", \"this\", \"war\", \",\", \"a\", \"danger\", \"there\", \"is\", \",\", \"of\", \"losing\", \"who\", \"we\", \"are\", \".\"], [\"many\", \"of\", \"the\", \"truths\", \"that\", \"we\", \"cling\", \"to\", \"depend\", \"on\", \"our\", \"point\", \"of\", \"view\", \".\"], [\"named\", \"must\", \"your\", \"fear\", \"be\", \"before\", \"banish\", \"it\", \"you\", \"can\", \".\"], [\"you\", \"think\", \"yoda\", \"stops\", \"teaching\", \",\", \"just\", \"because\", \"his\", \"student\", \"does\", \"not\", \"want\", \"to\", \"hear\", \"?\", \"a\", \"teacher\", \"yoda\", \"is\", \".\", \"yoda\", \"teaches\", \"like\", \"drunkards\", \"drink\", \",\", \"like\", \"killers\", \"kill\", \".\"], [\"do\", \"not\", \"assume\", \"anything\", \"obi-wan\", \".\", \"clear\", \"your\", \"mind\", \"must\", \"be\", \"if\", \"you\", \"are\", \"to\", \"discover\", \"the\", \"real\", \"villains\", \"behind\", \"this\", \"plot\", \".\"], [\"you\", \"will\", \"know\", \"(\", \"the\", \"good\", \"from\", \"the\", \"bad\", \")\", \"when\", \"you\", \"are\", \"calm\", \",\", \"at\", \"peace\", \".\", \"passive\", \".\", \"a\", \"jedi\", \"uses\", \"the\", \"force\", \"for\", \"knowledge\", \"and\", \"defense\", \",\", \"never\", \"for\", \"attack\", \".\"], [\"soon\", \"will\", \"i\", \"rest\", \",\", \"yes\", \",\", \"forever\", \"sleep\", \".\", \"earned\", \"it\", \"i\", \"have\", \".\", \"twilight\", \"is\", \"upon\", \"me\", \",\", \"soon\", \"night\", \"must\", \"fall\", \".\"], [\"when\", \"you\", \"look\", \"at\", \"the\", \"dark\", \"side\", \",\", \"careful\", \"you\", \"must\", \"be\", \".\", \"for\", \"the\", \"dark\", \"side\", \"looks\", \"back\", \".\"], [\"you\", \"will\", \"know\", \"(\", \"the\", \"good\", \"from\", \"the\", \"bad\", \")\", \"when\", \"you\", \"are\", \"calm\", \",\", \"at\", \"peace\", \".\", \"passive\", \".\", \"a\", \"jedi\", \"uses\", \"the\", \"force\", \"for\", \"knowledge\", \"and\", \"defense\", \",\", \"never\", \"for\", \"attack\", \".\"], [\"smaller\", \"in\", \"number\", \"are\", \"we\", \",\", \"but\", \"larger\", \"in\", \"mind\", \".\"], [\"your\", \"path\", \"you\", \"must\", \"decide\", \".\"], [\"always\", \"two\", \"there\", \"are\", \",\", \"no\", \"more\", \",\", \"no\", \"less\", \".\", \"a\", \"master\", \"and\", \"an\", \"apprentice\", \".\"], [\"no\", \"longer\", \"certain\", \",\", \"that\", \"one\", \"ever\", \"does\", \"win\", \"a\", \"war\", \",\", \"i\", \"am\", \".\", \"for\", \"in\", \"fighting\", \"the\", \"battles\", \",\", \"the\", \"bloodshed\", \",\", \"already\", \"lost\", \"we\", \"have\", \".\", \"yet\", \",\", \"open\", \"to\", \"us\", \"a\", \"path\", \"remains\", \".\", \"that\", \"unknown\", \"to\", \"the\", \"sith\", \"is\", \".\", \"through\", \"this\", \"path\", \",\", \"victory\", \"we\", \"may\", \"yet\", \"find\", \".\", \"not\", \"victory\", \"in\", \"the\", \"clone\", \"wars\", \",\", \"but\", \"victory\", \"for\", \"all\", \"time\", \".\"], [\"if\", \"no\", \"mistake\", \"you\", \"have\", \"made\", \",\", \"losing\", \"you\", \"are\", \".\", \"a\", \"different\", \"game\", \"you\", \"should\", \"play\", \".\"], [\"[\", \"luke\", \"skywalker\", \":\", \"]\", \"i\", \"can\", \"\\u2019\", \"t\", \"believe\", \"it\", \".\", \"[\", \"yoda\", \":\", \"]\", \"that\", \"is\", \"why\", \"you\", \"fail\", \".\"], [\"happens\", \"to\", \"every\", \"guy\", \"sometimes\", \"this\", \"does\"], [\"adventure\", \".\", \"excitement\", \".\", \"a\", \"jedi\", \"craves\", \"not\", \"these\", \"things\", \".\"], [\"only\", \"the\", \"dark\", \"lord\", \"of\", \"the\", \"sith\", \"knows\", \"of\", \"our\", \"weakness\", \".\", \"if\", \"informed\", \"the\", \"senate\", \"is\", \",\", \"multiply\", \"our\", \"adversaries\", \"will\", \".\"]]*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For roberta a location is needed where checkpoints and pre processing is saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = join(example_path[0], \"seaberta\", \"main\")\n",
    "train_path = join(example_path[0], \"seaberta\", \"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta needs some parameters. At least use 20 epochs for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "        \"per_gpu_eval_batch_size\": 4,\n",
    "        \"do_eval\": True,\n",
    "        \"evaluate_during_training\": False,\n",
    "        \"line_by_line\": False,\n",
    "        \"should_continue\": False,\n",
    "        \"model_name_or_path\": False,\n",
    "        \"mlm\": True,\n",
    "        \"do_train\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"overwrite_cache\": False,\n",
    "        \"block_size\": 512,\n",
    "        \"eval_all_checkpoints\": 2,\n",
    "        \"server_ip\": \"\",\n",
    "        \"mlm_probability\": 0.15,\n",
    "        \"local_rank\": -1,  # NO GPU,\n",
    "        \"no_cuda\": False,\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": 'O1',\n",
    "        \"max_steps\": 10,\n",
    "        \"warmup_steps\": 10,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"per_gpu_train_batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"max_grad_norm\": 100.0,\n",
    "        \"save_total_limit\": 10,\n",
    "        \"save_steps\": 10,\n",
    "        \"logging_steps\": 2,\n",
    "        \"seed\": 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = SeaBERTa(main_path, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:seaqube.nlp.roberta.roberta_training:Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Training new model from scratch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main.txt 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/modeling_auto.py:797: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Training/evaluation parameters {'per_gpu_eval_batch_size': 4, 'do_eval': True, 'evaluate_during_training': False, 'line_by_line': False, 'should_continue': False, 'model_name_or_path': False, 'mlm': True, 'do_train': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'block_size': 512, 'eval_all_checkpoints': 2, 'server_ip': '', 'mlm_probability': 0.15, 'local_rank': -1, 'no_cuda': False, 'fp16': False, 'fp16_opt_level': 'O1', 'max_steps': 10, 'warmup_steps': 10, 'learning_rate': 5e-05, 'per_gpu_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'weight_decay': 0.01, 'adam_epsilon': 1e-06, 'max_grad_norm': 100.0, 'save_total_limit': 10, 'save_steps': 10, 'logging_steps': 2, 'seed': 0, 'output_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output', 'model_type': 'roberta', 'config_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'tokenizer_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'train_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_train.txt', 'eval_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_eval.txt', 'cache_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/cache', 'n_gpu': 0, 'device': device(type='cpu')}\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Loading features from cached file /Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/roberta_cached_lm_510_main_train.txt\n",
      "INFO:seaqube.nlp.roberta.roberta_training:***** Running training *****\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Num examples = 3\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Num Epochs = 10\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Instantaneous batch size per GPU = 4\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Gradient Accumulation steps = 4\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Total optimization steps = 10\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/modeling_roberta.py:888: FutureWarning: The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer PreTrainedTokenizer(name_or_path='/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', vocab_size=335, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|██████████| 1/1 [00:13<00:00, 13.91s/it]\u001b[A\n",
      "Epoch:  10%|█         | 1/10 [00:13<02:05, 13.91s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.06s/it]\u001b[A\n",
      "Epoch:  20%|██        | 2/10 [00:27<01:51, 13.96s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\u001b[A\n",
      "Epoch:  30%|███       | 3/10 [00:42<01:37, 13.98s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\u001b[A\n",
      "Epoch:  40%|████      | 4/10 [00:56<01:23, 14.00s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.02s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 5/10 [01:10<01:10, 14.01s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.11s/it]\u001b[A\n",
      "Epoch:  60%|██████    | 6/10 [01:24<00:56, 14.04s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.07s/it]\u001b[A\n",
      "Epoch:  70%|███████   | 7/10 [01:38<00:42, 14.05s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\u001b[A\n",
      "Epoch:  80%|████████  | 8/10 [01:52<00:28, 14.05s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\u001b[A\n",
      "Epoch:  90%|█████████ | 9/10 [02:06<00:14, 14.05s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:14<00:00, 14.00s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 10/10 [02:20<00:00, 14.04s/it]\n",
      "INFO:seaqube.nlp.roberta.roberta_training: global_step = 1, average loss = 14.833906650543213\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Saving model checkpoint to /Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output\n",
      "/Users/allankarlson/.conda/envs/master-thesis/lib/python3.7/site-packages/transformers/modeling_auto.py:837: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "INFO:seaqube.nlp.roberta.roberta_training:Evaluate the following checkpoints: ['/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG {'per_gpu_eval_batch_size': 4, 'do_eval': True, 'evaluate_during_training': False, 'line_by_line': False, 'should_continue': False, 'model_name_or_path': False, 'mlm': True, 'do_train': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'block_size': 512, 'eval_all_checkpoints': 2, 'server_ip': '', 'mlm_probability': 0.15, 'local_rank': -1, 'no_cuda': False, 'fp16': False, 'fp16_opt_level': 'O1', 'max_steps': 10, 'warmup_steps': 10, 'learning_rate': 5e-05, 'per_gpu_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'weight_decay': 0.01, 'adam_epsilon': 1e-06, 'max_grad_norm': 100.0, 'save_total_limit': 10, 'save_steps': 10, 'logging_steps': 2, 'seed': 0, 'output_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/output', 'model_type': 'roberta', 'config_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'tokenizer_name': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main', 'train_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_train.txt', 'eval_data_file': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/main_eval.txt', 'cache_dir': '/Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/cache', 'n_gpu': 0, 'device': device(type='cpu'), 'train_batch_size': 4, 'num_train_epochs': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:seaqube.nlp.roberta.roberta_training:Loading features from cached file /Users/allankarlson/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/examples/seaberta/main/roberta_cached_lm_510_main_eval.txt\n",
      "INFO:seaqube.nlp.roberta.roberta_training:***** Running evaluation  *****\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Num examples = 0\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  Batch size = 4\n",
      "Evaluating: 0it [00:00, ?it/s]\n",
      "INFO:seaqube.nlp.roberta.roberta_training:***** Eval results  *****\n",
      "INFO:seaqube.nlp.roberta.roberta_training:  perplexity = tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "roberta.train(yoda_cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeaQuBeWordEmbeddingsModelSeaBERTa(SeaQuBeWordEmbeddingsModel):\n",
    "    def __init__(self, seaberta: SeaBERTa):\n",
    "        self.seaberta = seaberta\n",
    "\n",
    "    def vocabs(self):\n",
    "        return self.seaberta.wv.vocabs\n",
    "\n",
    "    @property\n",
    "    def wv(self):\n",
    "        return self.seaberta.wv\n",
    "\n",
    "    def word_vector(self, word):\n",
    "        return self.seaberta.wv[word]\n",
    "\n",
    "    def matrix(self):\n",
    "        return self.seaberta.wv.matrix\n",
    "\n",
    "    def context_embedding(self, words, position):\n",
    "        return self.seaberta.context_embedding(words, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaberta = SeaQuBeWordEmbeddingsModelSeaBERTa(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.29637682,  0.17495562, -0.32151467,  0.28507614,  0.36918864,\n",
       "        -0.68209875, -0.5761418 ,  0.0354082 ,  0.46975166, -0.51650286,\n",
       "         0.09801871,  0.10970846,  0.56172836,  0.58035207,  0.5220239 ,\n",
       "         0.23842059,  0.2670231 ,  0.3038025 ,  0.23341294, -0.90406716,\n",
       "         0.39077038, -0.43437737,  0.43068188, -0.22495733, -0.7062916 ,\n",
       "        -0.51804125, -0.01821473, -0.97982574, -0.44811812, -0.06091636,\n",
       "        -0.71079534, -0.43467906,  0.14559722, -0.09732482,  0.69507766,\n",
       "         0.12236589,  0.92346823,  1.1685666 ,  0.04599249,  0.36656547,\n",
       "         0.41244188, -0.5553379 ,  0.49144047,  0.19862236, -0.8497895 ,\n",
       "        -0.92402613,  0.42484593,  0.5010946 , -0.03935878,  0.8109755 ,\n",
       "         0.14996868,  0.01971561,  1.3662062 ,  0.84346354, -1.0190625 ,\n",
       "        -0.42705184, -0.41649067,  0.5931656 ,  0.2082949 ,  0.06140208,\n",
       "         0.35606593, -0.52583665,  0.09678748,  1.4051244 ,  0.69136775,\n",
       "         0.00541956,  0.34543437,  0.21721885, -0.41191185, -0.693739  ,\n",
       "         0.76795787, -0.52794176, -0.1741046 ,  1.2630928 , -0.9585964 ,\n",
       "         0.54211867,  0.75062144, -0.42942795,  0.7444888 , -0.13617305,\n",
       "         0.5747762 ,  0.37689865, -0.5295015 ,  0.5213802 , -1.015539  ,\n",
       "        -0.12008622, -0.7228955 ,  0.10790801,  0.62504476,  0.05650905,\n",
       "         0.95896554,  0.470127  ,  0.56765234,  0.84864765, -0.04619062,\n",
       "         0.10756448, -0.25046656,  0.09176052, -0.05009101, -0.0049451 ,\n",
       "         0.5788722 ,  0.29290447,  0.62716496, -0.6032784 ,  0.2985613 ,\n",
       "        -0.485519  , -0.56095314,  0.71672106,  0.44706288, -0.03946954,\n",
       "        -0.47753185,  0.00723597,  0.5273504 ,  0.65296865, -0.14290501,\n",
       "        -0.06387851,  0.86172974, -0.1670903 ,  0.424202  ,  0.3789014 ,\n",
       "         0.02710664, -0.38867122,  0.17093436,  0.55763936, -0.26216927,\n",
       "         0.5801304 ,  0.5215079 ,  0.6077869 , -0.77721596, -0.46218944,\n",
       "        -0.0180791 , -0.38860604, -0.66248536, -0.24203482,  0.4037699 ,\n",
       "        -0.970423  , -0.11337496,  0.29669446,  0.3724853 , -0.79476523,\n",
       "        -1.220459  ,  0.19330116, -0.12347754, -0.18956506, -0.06604013,\n",
       "        -0.834263  , -0.1470053 ,  0.5863685 ,  0.36450404,  0.42582402,\n",
       "        -0.69778335,  0.84002   ,  0.48831266, -0.42476767, -1.0775803 ,\n",
       "        -0.05850358, -0.9435519 ,  0.17631465,  0.6508573 , -1.0341408 ,\n",
       "        -1.0138651 ,  0.12890333,  0.25233582, -0.32299575,  0.843646  ,\n",
       "        -0.98389447, -0.09952813,  0.59550154,  0.7519181 ,  0.37252071,\n",
       "        -0.32075426, -0.04078969,  1.22815   , -0.05743897,  0.7038795 ,\n",
       "        -0.45682123,  0.0404166 , -0.55558044, -0.29752392, -1.2442    ,\n",
       "         0.51248276,  0.22169462, -0.14619927,  0.45727998,  0.45521936,\n",
       "         0.5196703 ,  0.90001243, -0.3436349 , -0.22706196,  0.6130686 ,\n",
       "        -0.45964855, -0.648777  ,  0.03797035, -0.25742435, -0.0470674 ,\n",
       "        -0.24489358, -0.5199094 ,  0.3592486 , -0.18705702,  0.30967486,\n",
       "         0.9436364 ,  0.12620914,  0.20568329, -0.32062677, -0.00991821,\n",
       "         0.03007217, -0.23175131,  0.6392413 , -0.33361632, -0.06848495,\n",
       "        -0.7250447 , -0.1531956 ,  1.511692  ,  0.3934949 ,  0.38060945,\n",
       "         0.66343546,  0.14919333,  1.2472775 , -0.41302496,  0.5555975 ,\n",
       "         0.9590727 , -0.07871929,  0.24562722,  0.9840021 , -0.3066445 ,\n",
       "         1.2514648 ,  0.54505885, -0.0611977 ,  0.5620348 ,  0.54559505,\n",
       "         0.0222038 ,  0.6035459 ,  0.2107566 , -0.1889227 ,  1.2538245 ,\n",
       "         0.34084883, -0.40055558, -0.11608721,  0.40579525, -0.3526868 ,\n",
       "         0.32417208,  0.41094247, -0.45326734,  0.488806  ,  0.32878834,\n",
       "         0.74310344, -0.2738479 ,  0.3475075 , -0.12433764,  0.26200685,\n",
       "        -0.04862355,  0.4833861 ,  0.86317015,  0.269833  , -0.7146584 ,\n",
       "        -0.17584896, -0.17521086,  0.15550534, -0.5390241 , -1.0988241 ,\n",
       "        -0.30282572,  0.10256895,  0.96475554, -0.37324762, -0.8010582 ,\n",
       "        -0.0814404 , -0.03719038, -0.5663887 ,  0.30117947,  0.5093586 ,\n",
       "        -0.30202448,  1.44971   , -0.35409886, -0.01458138,  1.0722502 ,\n",
       "        -0.5806509 ,  0.22135866, -0.12320575,  1.1026857 , -0.12242085,\n",
       "         0.3593575 ,  0.55020624, -0.69077265,  0.0571073 , -0.5357851 ,\n",
       "        -0.06444426,  0.47441435,  0.06646568, -1.2293372 , -0.6349687 ,\n",
       "        -0.7428781 , -1.0394086 ,  0.27750984, -1.1105931 , -0.41753638,\n",
       "         0.11182231,  0.4481252 ,  0.36186853, -0.27744544, -0.37990335,\n",
       "        -1.0636867 ,  0.71564686, -0.62071264, -0.03757944,  0.6616113 ,\n",
       "         0.52744424,  0.3527298 , -0.48482162,  0.16723797,  0.21879706,\n",
       "         0.12846062, -0.12234628, -0.0254662 , -1.1980842 ,  0.23067702,\n",
       "         1.281065  , -0.1895326 ,  0.46703058, -0.37707534, -0.40668738,\n",
       "        -0.40680742, -0.21703029,  0.258628  , -0.9496709 , -0.4812625 ,\n",
       "         1.3749312 ,  0.25354582, -0.4928071 ,  0.5227528 , -0.63093114,\n",
       "         0.07498407, -0.9391955 , -0.39328   , -0.23006476, -0.49586576]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.context_embedding([\"t\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 't',\n",
       " '.',\n",
       " 'th',\n",
       " 'a',\n",
       " 'i',\n",
       " ',',\n",
       " 'y',\n",
       " 'the',\n",
       " 'w',\n",
       " 'you',\n",
       " 'm',\n",
       " 'd',\n",
       " 'f',\n",
       " 's',\n",
       " 'o',\n",
       " 'h',\n",
       " 'l',\n",
       " 'b',\n",
       " 'c',\n",
       " 'to',\n",
       " 'p',\n",
       " 'is',\n",
       " 'no',\n",
       " 'for',\n",
       " 'j',\n",
       " 'be',\n",
       " 'k',\n",
       " 'in',\n",
       " 'wh',\n",
       " 'of',\n",
       " 'not',\n",
       " 'g',\n",
       " 'are',\n",
       " 'do',\n",
       " 'on',\n",
       " 'ha',\n",
       " 're',\n",
       " 'and',\n",
       " 'it',\n",
       " 'dar',\n",
       " 'know',\n",
       " 'have',\n",
       " 'dark']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.wv.vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "tin_can = RawModelTinCan(seaberta, word_count_list(yoda_cites))\n",
    "nlp = SeaQuBeNLPLoader.load_model_from_tin_can(tin_can, \"seaberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (3 of 3) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-49d6cb0cbf95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Yoda and more\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/seaqube/nlp/seaqube_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeaQuBeNLPToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-af9dbcdb4765>\u001b[0m in \u001b[0;36mcontext_embedding\u001b[0;34m(self, words, position)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcontext_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseaberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/01_UNI/CAPGEMINI/Capgemini_MASTER-THESIS/master-thesis/code/SeaQuBeRepo/seaqube/nlp/roberta/seaberta.py\u001b[0m in \u001b[0;36mcontext_embedding\u001b[0;34m(self, words, position)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mposition\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword_part\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp_orig_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0;31m#print(\"IN?\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0msummarized_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "nlp(\"Yoda and more\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
